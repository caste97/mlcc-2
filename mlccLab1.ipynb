{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLCC - Laboratory 1 - Local methods\n",
    "In this laboratory we will address the problem of data analysis with a reference to a classification problem. \n",
    "Follow the instructions below.\n",
    "\n",
    "Import all the functions from the file \"lab1ImpFunction.py\" by: <br>\n",
    "`from lab1ImpFunction import *` <br>\n",
    "Also import pyplot for plotting: <br>\n",
    "`import matplotlib.pyplot as plt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Warm up - data generation\n",
    "\n",
    "Loot at the function `MixGauss` in the file `lab1ImpFunction.py`\n",
    "* **1.A** The function  `MixGauss(means, sigmas, n)` generates dataset  `[X,Y]` where the `X` is composed of mixed classes, each class being generated according to a Gaussian distribution with given mean and standard deviation. The points in the dataset `X` are enumerated from 1 to n, and `Y` represents the label of each point. \n",
    "Hint: if the command help MixGauss fails, this probably means that you have not set up correctly your current working directory' \n",
    "Have a look at the code or, for a quick help, type \"help MixGauss\" on the Matlab shell.\n",
    "\n",
    "* **1.B** Type on a Jupyter Notebook ceel  the commands <br>\n",
    "`X, Y = MixGauss(means=[[0,0],[1,1]], sigmas=[0.5, 0.25], n=50)\n",
    "plt.figure(1)\n",
    "plt.title(\"Dataset 1\")\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=Y, alpha=0.8) %type \"help(plt.scatter)\" to see what the parameters mean\n",
    "plt.show()`\n",
    "\n",
    "* **1.C** Now generate a more complex dataset following the instructions below. This dataset will be referred hereafter as training dataset. \n",
    "Call MixGauss with appropriate parameters and produce a dataset with four classes: the classes must live in the 2D space and be centered on the corners of the unit square `(0,0), (0,1) (1,1), (1,0)`, all with standard deviation 0.3. \n",
    "The number of points in the dataset is up to you. \n",
    "`Xtr, C = MixGauss(....)`\n",
    "Use the pyplot function `scatter(...)` to plot the training dataset.\n",
    "Manipulate the data so to obtain a 2-class problem where data on opposite corners share the same class. \n",
    "If you produced the data following the centers order provided above, you may do: `Ytr = 2*np.mod(C, 2)-1`\n",
    "\n",
    "* **1.D** Following the same procedure as above (section 1.C) generate a new set of data `[Xte,Yte]` with the same distribution, hereafter called test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core - kNN classifier\n",
    "\n",
    "The k-Nearest Neighbors algorithm (kNN) assigns to a test point the most frequent label among its k closest points/examples in the training set. \n",
    "\n",
    "* **2.A** Have a look at the code of function kNNClassify (for a quick reference type \"help kNNClassify\" on the Matlab command prompt)\n",
    "\n",
    "* **2.B** Use kNNClassify on the previously generated 2-class data from section 1.D. Pick a \"reasonable\" k. Below we propose three ways of evaluating the quality of the prediction made by the kNN method. Try them and see the influence of the parameter k \n",
    "\n",
    "* **2.C1** [Evaluating the prediction] Plot the test data Xte twice. Once with its true labels Yte, and once with the predicted labels Ypred.  A possible way is:\n",
    "`plt.figure(3)\n",
    "%plot test points (filled circles) associating a different color to each \"true\" label\n",
    "plt.scatter(Xte[:,0], Xte[:,1], s=200, c=Yte, alpha=0.3, marker='o', edgecolog='black')\n",
    "% plot test points (empty circles) associating a different color to each estimated label\n",
    "plt.scatter(Xte[:,0], Xte[:,1], s=30, c=Ypred, alpha=1, marker='^')\n",
    "plt.show()`\n",
    "\n",
    "* **2.C2** [Evaluating the prediction] To compute the classification error percentage compare the estimated outputs with those previously  generated:\n",
    "`np.mean(Ypred ~= Yte)`\n",
    "\n",
    "* **2.C3** [Evaluating the prediction] To visualize the separating function, use the routine separatingFkNN. You may use help separatingFkNN in the command prompt or look directly at the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter selection - What is a good value for k?\n",
    "\n",
    "So far we considered an arbitrary k. We now want to inroduce different approaches for selecting it. \n",
    "\n",
    "* **3.A** Perform a hold out cross validation procedure on the available training data. \n",
    "You may want to use the function `holdoutCVkNN` available on the zip file (here again, type \"help holdoutCVkNN\" on the Matlab command prompt, you will find there a useful example of use).\n",
    "Plot the  training  and validation errors for the different values ok k. \n",
    "\n",
    "* **3.B** Add noise to the data by randomly flipping the labels on the training set, and call it `Ytr_noisy`. You can use the function flipLabels to do that. How does the validation error behave now with respect to k ? \n",
    "Note: Keep track of the best k , and the corresponding validation error. You will need it in 3.D. \n",
    "\n",
    "* **3.C** What happens with different values of p (percentage of points held out) and rep (number of repetitions of the experiment)?\n",
    "\n",
    "* **3.D** For now we have been using the training set to obtain a classifier. Now we want to evaluate its performance by applying it to an independent test set.\n",
    "Consider the test dataset `Xte,Yte` generated in point 1.E. Add some noise to the dataset by randomly flipping some labels from `Yte`. You can use the function `flipLabels` to create `Xte,Yte_noisy`.\n",
    "Take the best `k` you obtained by hold out cross validation in 3.B, and use it to get a prediction from `Xtr,Ytrnoisy,Xte`, as you did in part 2.\n",
    "Evaluate the prediction with respect to Yte noisy (as you did in 2.C2), and compare it to the validation error you had in 3.B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. If you have more time - More experiments\n",
    "\n",
    "* **4.A** Evaluate the results as the size of the training set grows. `n=10,20,50,100,300,...`  (of course `k` needs to be chosen accordingly)\n",
    "\n",
    "* **4.B** Generate more complex datasets with `MixGauss` function, for instance by choosing larger variance on the data generation part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
